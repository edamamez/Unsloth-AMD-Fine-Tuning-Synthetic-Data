{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the terms described in the LICENSE file in\n",
    "the root directory of this source tree.\n",
    "\n",
    "<a aria-label=\"Meta home\" href=\"https://www.llama.com/docs\" tabindex=\"0\" target=\"_blank\" >![Meta---Logo@1x.jpg](data:image/jpeg;base64,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)</a>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/getting-started/llama-tools/prompt-ops_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "# Getting Started with [prompt-ops](https://github.com/meta-llama/prompt-ops)\n",
    "\n",
    "This notebook will guide you through the process of using [prompt-ops](https://github.com/meta-llama/prompt-ops) to optimize your prompts for Llama models. We'll cover:\n",
    "\n",
    "1. Introduction to prompt-ops\n",
    "2. Setting up your environment\n",
    "3. Creating a sample project\n",
    "4. Running prompt optimization\n",
    "5. Analyzing the results\n",
    "6. Advanced usage and customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to prompt-ops\n",
    "\n",
    "### What is prompt-ops?\n",
    "\n",
    "prompt-ops is a Python package that **automatically optimizes prompts** for Llama models. It transforms prompts that work well with other LLMs into prompts that are optimized for Llama models, improving performance and reliability.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "prompt-ops takes three key inputs:\n",
    "1. Your existing system prompt\n",
    "2. A dataset of query-response pairs for evaluation and optimization\n",
    "3. A configuration file specifying model parameters and optimization details\n",
    "\n",
    "It then applies optimization techniques to transform your prompt into one that works better with Llama models, and provides metrics to measure the improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up your environment\n",
    "\n",
    "Let's start by installing the Prompt ops package and setting up our environment. We'll clone the repository and install it in editable mode to ensure we have the latest version, as the PyPI package is currently outdated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/rpdTracer-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/HipMarker-1.0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: llama-prompt-ops in /usr/local/lib/python3.12/dist-packages (0.0.8)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (8.3.0)\n",
      "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (2.32.5)\n",
      "Requirement already satisfied: dspy==2.6.13 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (2.6.13)\n",
      "Requirement already satisfied: numpy>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (1.15.2)\n",
      "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (2.3.3)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (1.1.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (6.0.3)\n",
      "Requirement already satisfied: tiktoken>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (0.9.0)\n",
      "Requirement already satisfied: openai>=1.65.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (2.3.0)\n",
      "Requirement already satisfied: litellm>=1.63.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (1.78.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.29.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (0.35.3)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (4.2.0)\n",
      "Requirement already satisfied: propcache==0.3.1 in /usr/local/lib/python3.12/dist-packages (from llama-prompt-ops) (0.3.1)\n",
      "Requirement already satisfied: backoff in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (1.5.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (2024.11.6)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (5.11.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (4.67.1)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (4.5.0)\n",
      "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (2.12.2)\n",
      "Requirement already satisfied: magicattr~=0.1.6 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (0.1.6)\n",
      "Requirement already satisfied: diskcache in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (5.6.3)\n",
      "Requirement already satisfied: json-repair in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (0.52.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (9.1.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (4.11.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (0.0.8)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (6.2.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from dspy==2.6.13->llama-prompt-ops) (3.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->llama-prompt-ops) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->llama-prompt-ops) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->llama-prompt-ops) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->llama-prompt-ops) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->llama-prompt-ops) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->llama-prompt-ops) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.21.0->llama-prompt-ops) (2025.9.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->llama-prompt-ops) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.29.0->llama-prompt-ops) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.29.0->llama-prompt-ops) (1.1.10)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->llama-prompt-ops) (3.13.0)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->llama-prompt-ops) (0.13.5)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->llama-prompt-ops) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->llama-prompt-ops) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->llama-prompt-ops) (4.23.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.63.0->llama-prompt-ops) (0.22.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.65.0->llama-prompt-ops) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.65.0->llama-prompt-ops) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.65.0->llama-prompt-ops) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->llama-prompt-ops) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->llama-prompt-ops) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->llama-prompt-ops) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->llama-prompt-ops) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->llama-prompt-ops) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->llama-prompt-ops) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->llama-prompt-ops) (2025.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->llama-prompt-ops) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->llama-prompt-ops) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->llama-prompt-ops) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->llama-prompt-ops) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->llama-prompt-ops) (6.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.63.0->llama-prompt-ops) (1.22.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.21.0->llama-prompt-ops) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.21.0->llama-prompt-ops) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.63.0->llama-prompt-ops) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.63.0->llama-prompt-ops) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->llama-prompt-ops) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->llama-prompt-ops) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.63.0->llama-prompt-ops) (0.23.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dspy==2.6.13->llama-prompt-ops) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dspy==2.6.13->llama-prompt-ops) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dspy==2.6.13->llama-prompt-ops) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->llama-prompt-ops) (1.17.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->dspy==2.6.13->llama-prompt-ops) (1.17.0)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna->dspy==2.6.13->llama-prompt-ops) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->dspy==2.6.13->llama-prompt-ops) (2.0.44)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->dspy==2.6.13->llama-prompt-ops) (1.3.10)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->dspy==2.6.13->llama-prompt-ops) (3.2.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository and install in editable mode\n",
    "!git clone https://github.com/meta-llama/prompt-ops.git\n",
    "!cd prompt-ops && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setting up your vllm server in another terminal\n",
    "\n",
    "`!vllm serve Unsloth/Llama-3.3-70B-Instruct   --port 8001   --max-model-len 48000   --gpu-memory-utilization 0.85`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Sample Project\n",
    "\n",
    "Prompt ops provides a convenient way to create a sample project with all the necessary files. Let's create a sample project to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/6] Creating project structure...\n",
      "✓ Created project directory: my-notebook-project\n",
      "✓ Created data directory\n",
      "✓ Created prompts directory\n",
      "\n",
      "[2/6] Generating configuration file...\n",
      "✓ Created config.yaml\n",
      "\n",
      "[3/6] Creating prompt template...\n",
      "✓ Created prompt.txt\n",
      "\n",
      "[4/6] Generating sample dataset...\n",
      "✓ Created dataset.json with 200 examples\n",
      "\n",
      "[5/6] Setting up environment...\n",
      "✓ Created .env file\n",
      "\n",
      "[6/6] Creating documentation...\n",
      "✓ Created README.md\n",
      "\n",
      "✨ Done! Project 'my-notebook-project' created successfully!\n",
      "\n",
      "To get started:\n",
      "1. cd my-notebook-project\n",
      "2. Edit the .env file to add your OPENROUTER_API_KEY\n",
      "   You can get an API key at: https://openrouter.ai/\n",
      "3. Run: llama-prompt-ops migrate\n"
     ]
    }
   ],
   "source": [
    "# Create a sample project\n",
    "!prompt-ops create my-notebook-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Important] Update config.yaml for vLLM on AMD\n",
    "\n",
    "```\n",
    "system_prompt:\n",
    "  file: prompts/prompt.txt\n",
    "  inputs:\n",
    "  - question\n",
    "  outputs:\n",
    "  - answer\n",
    "dataset:\n",
    "  path: data/dataset.json\n",
    "  input_field:\n",
    "  - fields\n",
    "  - input\n",
    "  golden_output_field: answer\n",
    "model:\n",
    "  task_model: hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
    "  proposer_model: hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
    "  api_base: http://localhost:8001/v1\n",
    "  temperature: 0.0\n",
    "  max_tokens: 4096\n",
    "metric:\n",
    "  class: llama_prompt_ops.core.metrics.FacilityMetric\n",
    "  strict_json: false\n",
    "  output_field: answer\n",
    "optimization:\n",
    "  strategy: llama\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a directory called `my-notebook-project` with a sample configuration and dataset. Let's explore the files that were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\n",
      "drwxr-xr-x  5 root root 4096 Oct 17 21:42 .\n",
      "drwxr-xr-x 12 root root 4096 Oct 17 21:42 ..\n",
      "-rw-r--r--  1 root root   48 Oct 17 21:42 .env\n",
      "drwxr-xr-x  2 root root 4096 Oct 17 21:42 .ipynb_checkpoints\n",
      "-rw-r--r--  1 root root  545 Oct 17 21:42 README.md\n",
      "-rw-r--r--  1 root root  526 Oct 17 21:42 config.yaml\n",
      "drwxr-xr-x  2 root root 4096 Oct 17 21:42 data\n",
      "drwxr-xr-x  2 root root 4096 Oct 17 21:42 prompts\n"
     ]
    }
   ],
   "source": [
    "!ls -la my-notebook-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample project includes:\n",
    "- `.env`: A file for your API key\n",
    "- `README.md`: Documentation for the project\n",
    "- `config.yaml`: Configuration file for prompt optimization\n",
    "- `data/dataset.json`: Sample dataset for evaluation and optimization\n",
    "- `prompts/prompt.txt`: Sample system prompt to optimize\n",
    "\n",
    "Let's examine the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_prompt:\n",
      "  file: prompts/prompt.txt\n",
      "  inputs:\n",
      "  - question\n",
      "  outputs:\n",
      "  - answer\n",
      "dataset:\n",
      "  path: data/dataset.json\n",
      "  input_field:\n",
      "  - fields\n",
      "  - input\n",
      "  golden_output_field: answer\n",
      "model:\n",
      "  task_model: hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
      "  proposer_model: hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
      "  api_base: http://localhost:8001/v1\n",
      "  temperature: 0.0\n",
      "  max_tokens: 4096\n",
      "metric:\n",
      "  class: llama_prompt_ops.core.metrics.FacilityMetric\n",
      "  strict_json: false\n",
      "  output_field: answer\n",
      "optimization:\n",
      "  strategy: llama\n"
     ]
    }
   ],
   "source": [
    "!cat my-notebook-project/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file specifies:\n",
    "- The system prompt to optimize\n",
    "- The dataset to use for evaluation and optimization\n",
    "- The model to use for optimization and evaluation\n",
    "- The metric to use for evaluation\n",
    "- The optimization strategy to use\n",
    "\n",
    "Let's also look at the sample prompt and dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. Extract and return a json with the following keys and values:\n",
      "- \"urgency\" as one of `high`, `medium`, `low`\n",
      "- \"sentiment\" as one of `negative`, `neutral`, `positive`\n",
      "- \"categories\" Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category is one of the best matching support category tags from: `emergency_repair_services`, `routine_maintenance_requests`, `quality_and_safety_concerns`, `specialized_cleaning_services`, `general_inquiries`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `cleaning_services_scheduling`, `customer_feedback_and_complaints`, `facility_management_issues`\n",
      "Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.\n"
     ]
    }
   ],
   "source": [
    "!cat my-notebook-project/prompts/prompt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Urgent Assistance Required for Specialized Cleaning Services\\n\\nDear ProCare Facility Solutions Support Team,\\n\\nI hope this message finds you well. My name is [Sender], and my family and I have been availing your services for our home for the past year. We have always appreciated the high standards and professionalism your team brings to maintaining our living environment.\\n\\nHowever, we are currently facing an urgent issue that requires immediate attention. We recently hosted a large gathering at our home, and despite our best efforts, there are several areas that now require specialized cleaning. Specifically, we need deep cleaning for our carpets and upholstery, as well as thorough window washing. The situation is quite pressing as we have more guests arriving soon, and we want to ensure our home is in pristine condition to welcome them.\\n\\nWe have tried some basic cleaning ourselves, but the results have not been satisfactory. Given the high standards we have come to expect from ProCare, we are confident that your team can handle this situation efficiently and effectively.\\n\\nCould you please arrange for a specialized cleaning team to visit our home at the earliest convenience? We would greatly appreciate it if this could be prioritized due to the urgency of the situation.\\n\\nThank you for your prompt attention to this matter. We look forward to your swift response and assistance.\\n\\nBest regards,\\n[Sender]\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": false}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"high\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Inquiry About Specialized Cleaning Services\\n\\nHi ProCare Support Team,\\n\\nI hope this message finds you well. My name is Alex, and I've been a client of ProCare Facility Solutions for a few months now. I must say, your services have been quite satisfactory so far, especially the routine maintenance and cleaning schedules.\\n\\nI am reaching out to inquire about your specialized cleaning services. Specifically, I am interested in deep cleaning and carpet maintenance for my residential property. While the regular cleaning has been great, I feel that a more thorough cleaning would really help maintain the pristine condition of my home.\\n\\nI haven't taken any steps yet to address this, as I wanted to get more information from your team first. Could you please provide me with details on how these specialized services work, the scheduling options available, and any additional costs involved?\\n\\nLooking forward to your response.\\n\\nBest regards,\\nAlex\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": false, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": true, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": true}, \\\"sentiment\\\": \\\"neutral\\\", \\\"urgency\\\": \\\"low\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"fields\": {\n",
      "      \"input\": \"Subject: Guidance Needed for Routine Plumbing Maintenance\\n\\nDear ProCare Support Team,\\n\\nI hope this message finds you well. My name is Dr. Samuel Thompson, and I have been a satisfied client of ProCare Facility Solutions for the past two years. Your commitment to quality and sustainability has always resonated deeply with my values, and I am grateful for the exceptional service your team consistently provides.\\n\\nI am writing to seek your assistance with a minor plumbing issue that has recently come to my attention. While it is not an urgent matter, I believe addressing it sooner rather than later would be beneficial. Specifically, there seems to be a small leak in the plumbing system of my office building. Although it has not caused any significant disruption, I would appreciate your expert guidance on how to proceed.\\n\\nIn an effort to mitigate the issue, I have already inspected the area and ensured that the immediate surroundings are dry and safe. However, given the importance of maintaining a well-functioning facility, I would like to request a professional assessment and any necessary routine maintenance at your earliest convenience.\\n\\nYour expertise and dedication to excellence have always been a source of reassurance for me, and I am confident that your team will handle this matter with the same level of care and professionalism that I have come to expect.\\n\\nThank you for your attention to this matter. I look forward to your prompt response and guidance.\\n\\nWarm regards,\\n\\nDr. Samuel Thompson\"\n",
      "    },\n",
      "    \"answer\": \"{\\\"categories\\\": {\\\"routine_maintenance_requests\\\": true, \\\"customer_feedback_and_complaints\\\": false, \\\"training_and_support_requests\\\": false, \\\"quality_and_safety_concerns\\\": false, \\\"sustainability_and_environmental_practices\\\": false, \\\"cleaning_services_scheduling\\\": false, \\\"specialized_cleaning_services\\\": false, \\\"emergency_repair_services\\\": false, \\\"facility_management_issues\\\": false, \\\"general_inquiries\\\": false}, \\\"sentiment\\\": \\\"positive\\\", \\\"urgency\\\": \\\"medium\\\"}\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 my-notebook-project/data/dataset.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Prompt Optimization\n",
    "\n",
    "Now that we have our sample project set up, let's run the prompt optimization process. We'll use the `migrate` command, which takes a configuration file as input and outputs an optimized prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env\n",
      "Loaded configuration from config.yaml\n",
      "2025-10-17 21:42:22,782 | INFO    |  Using model with DSPy: hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
      "Using the same model for task and proposer: hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
      "Using metric: FacilityMetric\n",
      "Resolved relative dataset path to: /jupyter-tutorial/AIAC_134_199_204_96/my-notebook-project/data/dataset.json\n",
      "Using dataset adapter: ConfigurableJSONAdapter\n",
      "2025-10-17 21:42:22,783 - root - WARNING - Model '' does not appear to be a Llama model. This library is optimized for Llama models.\n",
      "Auto-detected BasicOptimizationStrategy for model: \n",
      "2025-10-17 21:42:22,783 - root - WARNING - Model '<llama_prompt_ops.core.model.DSPyModelAdapter object at 0x75f3dd016b10>' does not appear to be a Llama model. This library is optimized for Llama models and may not work as expected.\n",
      "2025-10-17 21:42:22,784 - root - INFO - Loaded 200 examples from /jupyter-tutorial/AIAC_134_199_204_96/my-notebook-project/data/dataset.json\n",
      "2025-10-17 21:42:22,784 - root - INFO - Created dataset splits:\n",
      "2025-10-17 21:42:22,784 - root - INFO -   - Training:   50 examples (25.0% of total)\n",
      "2025-10-17 21:42:22,784 - root - INFO -   - Validation: 50 examples (25.0% of total)\n",
      "2025-10-17 21:42:22,784 - root - INFO -   - Testing:    100 examples (50.0% of total)\n",
      "Loaded prompt from file: /jupyter-tutorial/AIAC_134_199_204_96/my-notebook-project/prompts/prompt.txt\n",
      "Using 'system_prompt' from config\n",
      "Using config filename as output prefix: config\n",
      "Starting prompt optimization...\n",
      "2025-10-17 21:42:22,785 | INFO    | Applying BasicOptimizationStrategy to optimize prompt\n",
      "2025-10-17 21:42:22,785 | INFO    | Training set size: 50\n",
      "2025-10-17 21:42:22,785 | INFO    | Validation set size: 50\n",
      "2025-10-17 21:42:22,785 | INFO    | Test set size: 100\n",
      "\n",
      "Computing baseline score on 100 test examples using 18 threads...\n",
      "Average Metric: 79.17 / 100 (79.2%): 100%|████| 100/100 [01:10<00:00,  1.41it/s]\n",
      "2025/10/17 21:43:33 INFO dspy.evaluate.evaluate: Average Metric: 79.16666666666667 / 100 (79.2%)\n",
      "✅ Baseline Score: 79.170 in 71.08s\n",
      "\n",
      "2025-10-17 21:43:33,867 | INFO    | === Pre-Optimization Summary ===\n",
      "    Task Model       : hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
      "    Proposer Model   : hosted_vllm/Unsloth/Llama-3.3-70B-Instruct\n",
      "    Metric           : <llama_prompt_ops.core.metrics.FacilityMetric object at 0x75f3dd53c980>\n",
      "    Train / Val size : 50 / 50\n",
      "    MIPRO Params     : {\"auto_user\":\"basic\",\"auto_dspy\":\"light\",\"max_labeled_demos\":5,\"max_bootstrapped_demos\":4,\"num_candidates\":10,\"num_threads\":18,\"init_temperature\":0.5,\"seed\":9}\n",
      "    Baseline score   : 79.1700\n",
      "2025-10-17 21:43:33,867 - root - INFO - Optimization strategy using 5 labeled demos, 4 bootstrapped demos with 18 threads\n",
      "2025-10-17 21:43:33,867 - root - INFO - Compiling program with 50 training examples, 50 validation examples, and 100 test examples\n",
      "2025-10-17 21:43:33,868 - root - WARNING - Debug module not available, continuing without enhanced debugging\n",
      "2025-10-17 21:43:33,868 - root - INFO - Starting DSPy optimization with enhanced debugging\n",
      "2025-10-17 21:43:33,868 - root - INFO - Program type: <class 'dspy.predict.predict.Predict'>\n",
      "2025-10-17 21:43:33,868 - root - INFO - Trainset size: 50\n",
      "2025-10-17 21:43:33,868 - root - INFO - Valset size: 50\n",
      "2025-10-17 21:43:33,868 - root - INFO - First trainset example structure: <class 'dspy.primitives.example.Example'>\n",
      "2025-10-17 21:43:33,868 - root - WARNING - Example missing required 'inputs' or 'outputs' attributes\n",
      "2025-10-17 21:43:33,868 - root - WARNING - Example attributes: ['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_demos', '_input_keys', '_output_keys', '_store', 'copy', 'get', 'inputs', 'items', 'keys', 'labels', 'toDict', 'values', 'with_inputs', 'without']\n",
      "2025-10-17 21:43:33,868 - root - INFO - Calling optimizer.compile\n",
      "2025/10/17 21:43:33 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 7\n",
      "minibatch: False\n",
      "num_candidates: 5\n",
      "valset size: 50\n",
      "\n",
      "2025/10/17 21:43:33 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/10/17 21:43:33 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/10/17 21:43:33 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=5 sets of demonstrations...\n",
      "Bootstrapping set 1/5\n",
      "Bootstrapping set 2/5\n",
      "Bootstrapping set 3/5\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Run prompt optimization\n",
    "!cd my-notebook-project && prompt-ops migrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process will take a few minutes to complete. It involves:\n",
    "1. Loading your system prompt and dataset\n",
    "2. Analyzing the prompt structure and content\n",
    "3. Applying optimization techniques specific to Llama models\n",
    "4. Evaluating the optimized prompt against the original prompt\n",
    "5. Saving the optimized prompt to the `results/` directory\n",
    "\n",
    "Let's check the results directory to see the optimized prompt. If the optimizer successfully found a better prompt, it will be saved in the `results/` directory. You may need to run the optimization process again with different parameters or a larger dataset if the prompt is the same as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la my-notebook-project/results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized prompt is saved as a YAML file with a timestamp. Let's examine the contents of the optimized prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No result files found. Make sure the optimization process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import yaml\n",
    "\n",
    "# Find the most recent result file\n",
    "result_files = glob.glob('my-notebook-project/results/*.yaml')\n",
    "if result_files:\n",
    "    latest_result = max(result_files, key=os.path.getctime)\n",
    "    print(f\"Latest result file: {latest_result}\")\n",
    "    \n",
    "    # Load and display the optimized prompt\n",
    "    with open(latest_result, 'r') as f:\n",
    "        result = yaml.safe_load(f)\n",
    "        print(\"\\nOptimized System Prompt:\")\n",
    "        print(result.get('system', 'No system prompt found'))\n",
    "else:\n",
    "    print(\"No result files found. Make sure the optimization process completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Results\n",
    "\n",
    "Let's compare the original prompt with the optimized prompt to understand the changes made during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No result files found. Make sure the optimization process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the original prompt\n",
    "with open('my-notebook-project/prompts/prompt.txt', 'r') as f:\n",
    "    original_prompt = f.read()\n",
    "\n",
    "# Find the most recent result file again\n",
    "result_files = glob.glob('my-notebook-project/results/*.yaml')\n",
    "if result_files:\n",
    "    latest_result = max(result_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the optimized prompt\n",
    "    with open(latest_result, 'r') as f:\n",
    "        result = yaml.safe_load(f)\n",
    "        optimized_prompt = result.get('system', 'No system prompt found')\n",
    "    \n",
    "    # Print the comparison\n",
    "    print(\"Original Prompt:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(original_prompt)\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"\\nOptimized Prompt:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(optimized_prompt)\n",
    "    print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No result files found. Make sure the optimization process completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences in the Optimized Prompt\n",
    "\n",
    "The optimized prompt typically includes several improvements:\n",
    "\n",
    "1. **Better Structure**: Llama models respond better to clear, structured instructions\n",
    "2. **Llama-Specific Formatting**: Formatting that works better with Llama's training patterns\n",
    "3. **Few-Shot Examples**: Examples that help the model understand the expected output format\n",
    "4. **Clear Output Expectations**: More explicit instructions about what the output should look like\n",
    "\n",
    "These changes can significantly improve the model's performance on your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Usage and Customization\n",
    "\n",
    "### Using Your Own Data\n",
    "\n",
    "To use your own data with Prompt ops, you'll need to:\n",
    "\n",
    "1. Prepare your dataset in JSON format\n",
    "2. Create a system prompt file\n",
    "3. Create a configuration file\n",
    "\n",
    "Check out the comprehensive guide [here](https://github.com/meta-llama/prompt-ops/tree/main/docs) to learn more.\n",
    "\n",
    "Now, let's see how to create a custom configuration file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_config.yaml\n",
    "system_prompt:\n",
    "  file: \"path/to/your/prompt.txt\"\n",
    "  inputs: [\"question\"]\n",
    "  outputs: [\"answer\"]\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  path: \"path/to/your/dataset.json\"\n",
    "  input_field: \"question\"  # or [\"fields\", \"input\"] for nested fields\n",
    "  golden_output_field: \"answer\"\n",
    "\n",
    "# Model configuration\n",
    "model:\n",
    "  task_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "  proposer_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "# Metric configuration\n",
    "metric:\n",
    "  class: \"prompt_ops.core.metrics.StandardJSONMetric\"\n",
    "  strict_json: false\n",
    "  output_field: \"answer\"\n",
    "\n",
    "# Optimization settings\n",
    "optimization:\n",
    "  strategy: \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different Metrics\n",
    "\n",
    "Prompt ops supports different metrics for evaluating prompt performance. The default is `StandardJSONMetric`, but you can use other metrics like `FacilityMetric` for specific use cases.\n",
    "\n",
    "Here's an example of using the `FacilityMetric` for the facility support analyzer use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing facility_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile facility_config.yaml\n",
    "system_prompt:\n",
    "  file: \"prompts/facility_prompt.txt\"\n",
    "  inputs: [\"question\"]\n",
    "  outputs: [\"answer\"]\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  path: \"data/facility_dataset.json\"\n",
    "  input_field: [\"fields\", \"input\"]\n",
    "  golden_output_field: \"answer\"\n",
    "\n",
    "# Model configuration\n",
    "model:\n",
    "  name: \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "  task_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "  proposer_model: \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "# Metric configuration\n",
    "metric:\n",
    "  class: \"prompt_ops.core.metrics.FacilityMetric\"\n",
    "  strict_json: false\n",
    "  output_field: \"answer\"\n",
    "\n",
    "# Optimization settings\n",
    "optimization:\n",
    "  strategy: \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different Models\n",
    "\n",
    "Prompt ops supports different models through various inference providers. You can use OpenRouter, vLLM, or NVIDIA NIMs depending on your infrastructure needs.\n",
    "\n",
    "Here's an example of using a different model through OpenRouter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing different_model_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile different_model_config.yaml\n",
    "system_prompt:\n",
    "  file: \"prompts/prompt.txt\"\n",
    "  inputs: [\"question\"]\n",
    "  outputs: [\"answer\"]\n",
    "\n",
    "# Dataset configuration\n",
    "dataset:\n",
    "  path: \"data/dataset.json\"\n",
    "  input_field: \"question\"\n",
    "  golden_output_field: \"answer\"\n",
    "\n",
    "# Model configuration\n",
    "model:\n",
    "  name: \"openrouter/meta-llama/llama-3.1-8b-instruct\"\n",
    "  task_model: \"openrouter/meta-llama/llama-3.1-8b-instruct\"\n",
    "  proposer_model: \"openrouter/meta-llama/llama-3.1-8b-instruct\"\n",
    "\n",
    "# Metric configuration\n",
    "metric:\n",
    "  class: \"prompt_ops.core.metrics.StandardJSONMetric\"\n",
    "  strict_json: false\n",
    "  output_field: \"answer\"\n",
    "\n",
    "# Optimization settings\n",
    "optimization:\n",
    "  strategy: \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. Introduction to Prompt ops and its benefits\n",
    "2. Creating a sample project\n",
    "3. Setting up your environment and API key\n",
    "4. Running prompt optimization\n",
    "5. Analyzing the results\n",
    "6. Advanced usage and customization options\n",
    "\n",
    "Prompt ops provides a powerful way to optimize your prompts for Llama models, improving performance and reliability. By following the steps in this notebook, you can start optimizing your own prompts and building more effective LLM applications.\n",
    "\n",
    "For more information, check out the [prompt-ops documentation](https://github.com/meta-llama/prompt-ops/tree/main/docs) and explore the example use cases in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
